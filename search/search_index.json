{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pallet Insight ROS2 Package","text":"Pallet Insight ROS2 is a pallet detection and segmentation ROS2 package designed for real-time pallet identification in warehouse environments. This solution primarily uses RGB images, with optional parameters to incorporate depth images for enhanced model performance. The entire pipeline is optimized for edge devices, specifically the NVIDIA Jetson Orin ecosystem, to support mobile robotic applications.  Pallet Insight ROS2 Node with Dummy Publisher"},{"location":"data/","title":"Dataset Acquisition and Preparation","text":"Object Detection Dataset [Roboflow] Instance Segmentation Dataset [Roboflow]"},{"location":"data/#step-1-annotate-dataset","title":"Step 1: Annotate Dataset","text":"The Pallet Dataset contains 519 images, which were annotated as the first step. To expedite the labeling process, I used auto-labeling with Grounding DINO, integrated directly in Roboflow for object detection dataset and SAM2 for segmentation dataset, which significantly improved efficiency.   Note: While Grounding DINO performed well in identifying the ground, it encountered challenges in accurately labeling pallets. It often inferred both the pallet and the payload as a single entity. For SAM2, while it is near to accurate for gound I found using basic polygon tool more efficient for pallets."},{"location":"data/#step-2-dataset-split","title":"Step 2: Dataset Split","text":"<p>For the initial testing phase, the dataset was split as follows:</p> <ul> <li>Training: 70%</li> <li>Validation: 20%</li> <li>Testing: 10%</li> </ul> <p>Note: In the future, we aim to implement k-fold cross-validation for more robust model evaluation.</p>"},{"location":"data/#step-3-data-augmentation","title":"Step 3: Data Augmentation","text":"<p>Using the dataset export feature, the 519 images were augmented with the following parameter variations:</p> <ul> <li>Saturation: Between -25% to +25%</li> <li>Brightness: Between -25% to +25%</li> <li>Exposure: Between -15% to +15%</li> </ul>  Note: These ranges were set to the maximum permissible limits before Roboflow suggested adjustments, to establish a baseline model. Future enhancements may involve additional augmentation to improve model robustness."},{"location":"data/#step-4-dataset-for-training","title":"Step 4: Dataset for Training","text":"<p>Dataset consists following distribution:</p> <ul> <li>Training set: 1092</li> <li>Validation set: 103</li> <li>Test set: 52 </li> </ul>"},{"location":"deploy/","title":"Edge Deployment Optimization","text":""},{"location":"install/","title":"Installation","text":"Pallet Insight ROS2 Package can be installed locally or used in a Docker container. Current documentation is limited to Docker workflow which involves the usage of NVIDIA Container Toolkit, details to which can be found [here]."},{"location":"install/#arm64-architecture-jetson-orin","title":"arm64 architecture (Jetson Orin)","text":"<p>Pull the docker image of Pallet Insight ROS2 Package with arm64 tag from Docker Hub: <pre><code>docker pull trushant99/pallet_insight:arm64\n</code></pre></p> <p>Run the container with dummy publisher node (make sure docker-compose.yaml file uses dummy_pub.sh): <pre><code>docker compose up\n</code></pre></p> <p>Run the container with external data (make sure docker-compose.yaml file uses entrypoint.sh)): <pre><code>docker compose up \n</code></pre></p>"},{"location":"install/#x86-architecture-nvidia-gpu","title":"x86 architecture (Nvidia GPU)","text":"<p>Coming Soon ...</p>"},{"location":"install/#system-requirements","title":"System Requirements","text":"<p>The current docker image is built for JetPack 6.1 with Jetson Linu 36.4, featuring Linux Kernel 5.15 and an Ubuntu 22.04 based root file system. Refer [here] to flash your Jetson devices with latest JetPack.</p>  In case you are using the JetPack 6.0 which is built on Jetson Linux 36.3 and would like to use compute stack of JetPack 6.1, please refer [here]."},{"location":"model/","title":"Object Detection and Semantic Segmentation","text":""},{"location":"model/#object-detection-model","title":"Object Detection Model","text":"<p>Currently, the model used for detecting pallets and ground is YOLOv11. This model was chosen due to its balance between performance and efficiency. Despite utilizing fewer parameters, it achieves a commendable mAP (mean Average Precision) score, making it a suitable choice for edge computing devices like Jetson Orin boards. Additionally, its architecture is easier to optimize and quantize, which is critical for resource-constrained environments.</p>"},{"location":"model/#model-selection","title":"Model Selection","text":"<p>The YOLOv11 model is available in multiple sizes. Considering the requirements for edge deployment, I selected the small and medium-sized variants, as they offer an optimal trade-off between accuracy and computational efficiency.</p> <p>Below are the training and validation results for both model sizes:</p>"},{"location":"model/#1-yolov11-small","title":"1. YOLOv11-Small:","text":"Figure 1: Training Result (100 epoch) Figure 2: Training Confusion Matrix Normalized Figure 3: Validation Confusion Matrix Normalized"},{"location":"model/#2-yolov11-medium","title":"2. YOLOv11-Medium:","text":"Figure 4: Training Result (100 epoch) Figure 5: Training Confusion Matrix Normalized Figure 6: Validation Confusion Matrix Normalized <p>Note: The results shown above pertain to the YOLOv11 small and medium-sized models trained for 100 epochs. For a comprehensive analysis, including results from both 50-epoch and 100-epoch training, refer to the full results.</p> <p>Download the complete result: (YOLOv11s-50ep) (YOLOv11s-100ep) (YOLOv11m-50ep) (YOLOv11m-100ep)</p>"},{"location":"ros2/","title":"ROS2 Node Development","text":""}]}